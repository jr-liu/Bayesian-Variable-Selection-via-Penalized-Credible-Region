## Summary of the Project
For this project, I will be reviewing “Consistent high-dimensional Bayesian variable selection via penalized credible regions” (Bondell and Reich, 2012) and using “Priors for Bayesian shrinkage and high-dimension model selection” (Shin, 2017) as a supplementary source. This paper studies a new Bayesian variable selection method in linear regression models called penalized credible regions approach. The second paper provides some instructions on the dataset that I will be using and lists some methods of performance measurement. I will be extensively summarizing the selected paper into four parts: introduction, penalized credible regions, asymptotic theory, and simulations. Next, I will be implementing two types of the proposed approach, the joint sets and the marginal sets approach, on Boston housing dataset in varying settings and compare the results with those of the traditional Lasso.

## Introduction of the Project
With the increasing dimensions of modern data and necessity to reduce dimensions, variable selection is a topic gaining popularity. Statisticians have designed a variety of methods for variable selection, including both frequentist and Bayesian approaches. Some frequentist methods include forward selection, backward elimination, least absolute shrinkage and selection operator, fused Lasso, adaptive Lasso and so on. On the other hand, the Bayesian statistics most frequently approaches the variable selection problems with a probability-based MCMC sampling call Stochastic Search Variable Selection (SSVS), which we focused a lot in class and assignments. There are many papers discussing the pros and cons of each method with respect to accuracy, computation time, whether predictors are correlated or not, dimension relative to sample size and so on. Finding the most suitable variable selection method for a specific dataset is always a debatable topic. The paper I selected is the same. This paper suggests that the penalized credible regions approach is a more efficient and more accurate method than other commonly used Bayesian and frequentist approach, and it exhibits selection consistency in high dimensional settings. Therefore, I choose to review this paper to see how the proposed approach differs from other else. The Boston housing dataset is then selected because it is frequently used to study variable selection.

## Extensive Description of the Paper
### Part I: Introduction
The paper begins with reviewing traditional Bayesian variable selection. In high-dimensional settings, where $p \gg n$, since only an unknown subset of $\beta$ is truly relevant, the traditional Bayesian approach to variable selection places a prior on $\beta$, and then proceeds by indexing candidate predictors using a vector $\delta = (\delta_1, \delta_2, \delta_3)$ and assigning each $\delta_j$ a binary outcome $0$ or $1$ based on whether the candidate predictor should be included in the model or not. The posterior probabilities of each combination of predictors can be computed afterward, and the model with the greatest posterior probability is intuitively the ideal one. Sometimes, it is not feasible to directly calculate the posterior probabilities of $2^p$ models, so Stochastic Search Variable Selection (SSVS) via MCMC chains is further developed. When $p$ is large, the marginal probability of inclusion for each variable gives better insight into which predictors to include than searching for the highest posterior probability model, due to the fact that the posterior probability of every single model will be close to $0$ as the number of models becomes sufficiently large.

Although the traditional Bayesian variable selection methods (both direct computa- tion and SSVS) are widely adopted, there are several notable drawbacks. First, the method depends heavily on the choices of priors, while in a high dimensional setting, there is almost no clue to search for prior information. Second, we need to specify a prior for inclusion probabilities πj. Third, the result relies on the choices of a posterior threshold that determines which variables to include. Furthermore, MCMC is computationally expensive. In practice, looping through a 2p model space takes a significant amount of time. Another point to notice is that the marginal probabilities offer a poor indication if variables are highly correlated since correlated variables have an almost equal chance to show up.

Hence, the paper proposes a new way for Bayesian variable selection called penalized credible regions approach, which resolves the problems of the traditional methods. The proposed method separates the variable selection and model fitting. One subcategory of it is the joint credible regions approach, which shows selection consistency except in ultra-high dimensional settings, i.e. p≫n. Here, consistency implies that as napproaches ∞, the true variables are selected with a probability tending to 1. Another subcategory marginal credible regions approach displays the same trait even when p diverges faster than n (p≫n). The selection consistency will be explored in detail later in Part III. The proposed approach consistently outperforms the SSVS approach in high dimensional settings, and even for p<n, placing a flat prior on β allows the proposed approach to yield result similar to that of its frequentist counterpart adaptive Lasso. In addition, when p ≈n, the proposed approach gives a much better performance than the frequentist methods.

### Part II: Penalized Credible Regions
The paper further elaborates on the penalized credible regions approach and demonstrates some computational aspects. In a linear regression model, Y= XTβ+ ϵ, the general idea of the proposed method is to build a (1−α) ×100% credible region Cα for β. Cα is commonly assumed to be the highest posterior density (HPD) region. Accordingly, one can consider all the points within the credible region as feasible estimates for β. If 0 is included in the credible region for β, we remove the corresponding predictor xfrom the model. Bringing this idea to a higher dimensional setting, if we desire to choose the model with the sparsest representation, we can select the elements of Cα with the most zeros. The two plots below illustrate the case of dropping both x1 and x2 and dropping x1 but keeping x2 respectively when p= 2. The plots are extracted from the online slides prepared by the same authors of this paper.

## My Work and Extensions
In this part, I applied the penalized credible regions approach on Boston housing dataset in a variety of settings, compared the outcomes with the frequentist Lasso approach, and concluded a list of variables selected using each method. The Boston housing dataset has a sample size of 506 and a dimension of 14. Among them, there are 13 predictors (crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, and lstat) and 1 response variable (medv), which is the median value of owner-occupied homes in $1,000s. Because there are only 13 predictors, we need to generate spurious irrelevant variables for variable selection (Shin, 2017). Hence, I decided to generate noise variables ∈{20, 400}to bring the dimension of the dataset to {33, 413}, representing a low dimension and a high dimension respectively. Unlike the simulations in the paper, I only use this single one dataset instead of simulating multiple datasets. Furthermore, I am curious about how independence and correlations among covariates influence the accuracy of each method, so I sampled both independent noise variables from a standard normal distribution and correlated noise variables from N(0, 1) with AR(1) and lag-1 correlation of ρ = 0.5. To measure performance, I used the ROC curve mentioned in the paper. Also, I incorporated two new measurements, MS-O and MS-N, which count the average number of selected original variables and selected noise variables respectively of the first 20 ordered sets from the complete solution path.

For the credible sets approach, I utilized an online package called ”BayesPen” and input data, hyperparameters of priors, number of iterations equaling 5,000, and number of burn-in equaling 500. The residuals are assumed to be identically and independently distribution as N(0, τ) with τ ∼Gamma(0.01, 0.01). The regression coefficients also has a N(0, τ) distribution with τ ∼Gamma(0.01, 0.01). The function “BayesPen.lm” automatically generated ordered sets, and the outputs offered a complete solution path of adding variables (from zero to all variables) for both joint and marginal approaches. For the Lasso regression, I simply used the “cv.glmnet” function with varying tuning parameter λ to create ordered sets.

Subsequently, I plotted the ROC curves of the complete solution paths through computing FPR and TPR. However, when calculating MS-O and MS-N, I used only the first 20 ordered sets from the complete paths. The first 20 ordered sets contain the variables that were added to the model the earliest. Ideally, the first 20 ordered sets should contain the 13 true variables. Below are the ROC plots under each setting.

![4](https://github.com/user-attachments/assets/db2586b7-7ac8-4dfc-8e7b-c79390bae6d9)

![5](https://github.com/user-attachments/assets/482651dc-dc3a-46e3-8346-913fb071291e)

![6](https://github.com/user-attachments/assets/4e1a406c-63d2-485f-a411-0748908e2b11)

![7](https://github.com/user-attachments/assets/e169a7f6-2904-489c-a0cd-8411252e7c81)

From the above plots, we can tell in most cases, the joint and marginal credible regions approaches have similar ROC curves and area under the curve (AUC). The joint approach performs slightly better than the marginal approach when variables are not correlated. Increasing dimensions and correlation does not reduce the AUC for these two methods. On the other hand, the Lasso approach underperforms both credible sets approaches most of the time, especially given a higher-dimensional setting, but it is surprising to see that in a low dimensional setting, when the correlation increases from 0 to 0.5, the Lasso regression does not perform worse. This finding somehow contradicts the statement in the paper that Lasso suffers greatly from correlations among variables. Hence, this contradiction is worth further studying. After that, I created a table for MS-O and MS-N.

![t1](https://github.com/user-attachments/assets/9764a7c2-3bc4-47ce-9a4e-1843ce812df8)

Table 1 compares the MS-O and the MS-N for the first 20 ordered sets under each setting. The marginal method keeps selecting the most original predictors in all cases, followed by the joint approach, but the marginal approach also has a higher MS-N than the joint one. Moreover, I notice that when pis large, the MS-O of credible sets methods improves and the MS-N drops as we increase the correlation. This suggests that the proposed approach yields an even more accurate outcome using correlated predictors. In a low dimensional setting, the Lasso regression always selects the fewest original variables and the most noise variables. With a higher dimension, the average numbers of selected original variables of Lasso are down to 1.55 and 1.6, which are very low and unreasonable. Meanwhile, the MS-N is down to 0 for both independent and correlated variables. This result again assures us that Lasso is not as suitable as the credible sets approach when p is large.

Regarding the computation time, when pis small, the output of Lasso regression was generated simultaneously as I run the code, while the credible sets methods took less than 5 seconds. When I increase the dimension to 413, the frequentist Lasso took a few second, and the credible sets method took less than 20 seconds. In general, the proposed approach took longer time.

![WeChat65f90c3610079b261f801c58daaabe4e](https://github.com/user-attachments/assets/f46cbb10-cca5-455e-bb74-5e3988282f3c)

Next, in order to find out the exact variables selected, I examined the order path of each method. For convenience, I only selected the independent cases under low dimension and high dimension to study. A table including the first 13 variables added is shown above. Among these variables, “lstat”, “rm”, “ptratio”, “dis”, “nox”, “black”, “tax” frequently appear at the beginning. These variables correspond to % lower status of the population, average number of rooms per dwelling, pupil-teacher ratio by town, weighted distances to five Boston employment centers, nitric oxides concentration (parts per 10 million), 1000(Bk - 0.63)2 where Bk is the proportion of blacks by town, and full-value property-tax rate per $10,000. All methods selected a few noise variables, with the marginal approach in a high dimensional setting selecting the most. Among those original variables, “age” was never selected, indicating that proportion of owner-occupied units built prior to 1940 is not a necessary element determining the median value of owner-occupied homes in $1000’s. Moreover, we do notice that the marginal order path in high dimension is slightly different from the remaining three.

## Conclusion
In conclusion, for this project, I extensively summarized the paper of my choice and demonstrated how credible regions approach could be a more efficient method than other frequentist methods and SSVS. Then, I extended the paper by including my own application on Boston housing dataset using the proposed method and frequentist Lasso. In general my own application correspond to the statements in the paper. The penalized credible regions approach works well in both low dimension and high dimension. With an increase in correlation among variables, the proposed approach yields an even more desirable MS-O and MS-N. On the contrary, the frequentist Lasso has a poor performance when the dimension increases. However, in my experiment, the performance of Lasso did not worsen much as I switch from independent variables to correlated variables. This point is worth further study. Finally, in this paper, I only compared the proposed method with the Lasso regression. For further studies, I could consider fitting the same dataset using MCMC and other common methods for a more comprehensive comparison.
