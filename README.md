## Summary of the Project
For this project, I will be reviewing “Consistent high-dimensional Bayesian variable selection via penalized credible regions” (Bondell and Reich, 2012) and using “Priors for Bayesian shrinkage and high-dimensional model selection” (Shin, 2017) as a supplementary source. The first paper studies a new Bayesian variable selection method in linear regression models called the penalized credible regions approach. The second paper provides some instructions on the dataset that I will be using and lists methods for performance measurement. I will be extensively summarizing the selected paper into four parts: introduction, penalized credible regions, asymptotic theory, and simulations. Next, I will be implementing two types of the proposed approach — the joint credible sets and the marginal credible sets approaches — on the Boston housing dataset in varying settings and comparing the results with those of the traditional Lasso.

## Introduction of the Project
With the increasing dimensions of modern data and the necessity to reduce dimensions, variable selection is a topic gaining popularity. Statisticians have designed a variety of methods for variable selection, including both frequentist and Bayesian approaches. Some frequentist methods include forward selection, backward elimination, least absolute shrinkage and selection operator (Lasso), fused Lasso, adaptive Lasso, and so on. On the other hand, Bayesian statistics most frequently approaches variable selection problems with a probability-based MCMC sampling method called Stochastic Search Variable Selection (SSVS), which we focused on a lot in class and assignments. There are many papers discussing the pros and cons of each method with respect to accuracy, computation time, whether predictors are correlated or not, dimension relative to sample size, and so on. Finding the most suitable variable selection method for a specific dataset is always a debatable topic. The paper I selected is the same. This paper suggests that the penalized credible regions approach is a more efficient and more accurate method than other commonly used Bayesian and frequentist approaches, and it exhibits selection consistency in high-dimensional settings. Therefore, I chose to review this paper to see how the proposed approach differs from others. The Boston housing dataset is then selected because it is frequently used to study variable selection.

## Extensive Description of the Paper
### Part I: Introduction
The paper begins with reviewing traditional Bayesian variable selection. In high-dimensional settings, where $p \gg n$, since only an unknown subset of $\beta$ is truly relevant, the traditional Bayesian approach to variable selection places a prior on $\beta$, and then proceeds by indexing candidate predictors using a vector $\delta = (\delta_1, \delta_2, \delta_3)$ and assigning each $\delta_j$ a binary outcome $0$ or $1$ based on whether the candidate predictor should be included in the model or not. The posterior probabilities of each combination of predictors can be computed afterward, and the model with the greatest posterior probability is intuitively the ideal one. Sometimes, it is not feasible to directly calculate the posterior probabilities of $2^p$ models, so Stochastic Search Variable Selection (SSVS) via MCMC chains is further developed. When $p$ is large, the marginal probability of inclusion for each variable gives better insight into which predictors to include than searching for the highest posterior probability model, due to the fact that the posterior probability of every single model will be close to $0$ as the number of models becomes sufficiently large.

Although traditional Bayesian variable selection methods (both direct computation and SSVS) are widely adopted, there are several notable drawbacks. First, the method depends heavily on the choice of priors, while in a high-dimensional setting there is almost no guidance for selecting appropriate prior information. Second, we need to specify a prior for the inclusion probabilities $\pi_j$. Third, the results rely on the choice of a posterior threshold that determines which variables to include. Furthermore, MCMC is computationally expensive. In practice, looping through a $2^p$ model space takes a significant amount of time. Another point to note is that marginal inclusion probabilities provide a poor indication when variables are highly correlated, since correlated variables have nearly equal chances of being selected.

Hence, the paper proposes a new Bayesian variable selection method called the penalized credible regions approach, which resolves several problems of traditional methods. The proposed method separates variable selection from model fitting. One subcategory is the joint credible regions approach, which exhibits selection consistency except in ultra–high-dimensional settings, i.e., $p \gg n$. Here, consistency means that as $n \to \infty$, the true variables are selected with probability tending to $1$. Another subcategory, the marginal credible regions approach, displays the same property even when $p$ diverges faster than $n$ (i.e., $p \gg n$). The selection consistency will be explored in detail later in Part III. The proposed approach consistently outperforms the SSVS approach in high-dimensional settings, and even when $p < n$, placing a flat prior on $\beta$ allows the proposed approach to yield results similar to those of its frequentist counterpart, the adaptive Lasso. In addition, when $p \approx n$, the proposed approach delivers substantially better performance than frequentist methods.

### Part II: Penalized Credible Regions
The paper further elaborates on the penalized credible regions approach and demonstrates several computational aspects. In a linear regression model, $Y = X^\top \beta + \epsilon$, the general idea of the proposed method is to construct a $(1 - \alpha) \times 100$% credible region $C_\alpha$ for $\beta$. The region $C_\alpha$ is commonly assumed to be the Highest Posterior Density (HPD) region. Accordingly, all points within the credible region can be regarded as feasible estimates for $\beta$. If $0$ is included in the credible region for a component of $\beta$, the corresponding predictor $x$ is removed from the model. Extending this idea to a higher-dimensional setting, if the goal is to select the model with the sparsest representation, one can choose the elements of $C_\alpha$ that contain the largest number of zeros. The two plots below illustrate the cases of dropping both $x_1$ and $x_2$, and dropping $x_1$ while retaining $x_2$, respectively, when $p = 2$. The plots are extracted from the online slides prepared by the same authors of this paper.

To be more specific, for the proposed approach, only the parameters in the full model are assigned prior distributions. Thus, priors are specified only for $\beta$ and $\sigma^2$ (usually simple conjugate priors), with $\beta \sim \mathcal{N}\left(0, \frac{\sigma^2}{\tau} I_p \right)$ and $\tau \sim \mathrm{Gamma}(\alpha, \beta)$. The quantity $\lVert \beta \rVert_0$ is defined as the $\ell_0$ norm of the vector, which equals the number of nonzero elements in $\beta$. The sparsest model is obtained as $\tilde{\beta} = \arg\min_{\beta} \lVert \beta \rVert_0$ subject to $\beta \in C_\alpha$, that is, the element of $C_\alpha$ with the minimum number of nonzero components. The selected model at a given $\alpha$ is then denoted by $A_n^\alpha = \\{ j : \tilde{\beta_{j}} \neq 0 \\}$. If $\tau$ is fixed, the joint credible region for $\beta$ is elliptical and takes the form $C_\alpha = \\{ \beta : (\beta - \hat{\beta})^\top \Sigma^{-1} (\beta - \hat{\beta}) \le C_\alpha \\}$, where $\hat{\beta}$ and $\Sigma$ denote the posterior mean of $\beta$ and the posterior covariance matrix, respectively, both of which admit closed-form expressions. If $\tau$ is instead assigned a prior distribution, a short MCMC run is required to obtain $\hat{\beta}$ and $\Sigma$. Although the resulting credible region is no longer elliptical, it is still considered valid.

However, the combinatorial search and non-unique solutions both increase the difficulty of finding the sparsest estimates. In this case, the paper introduces a smooth homotopy between $\ell_0$ and $\ell_1$, given by $\sum_{j=1}^{p} \rho_a(|\beta_j|)$, in place of $\ell_0$.

$$
\rho_a(t) = \frac{(a+1)t}{a+t} = \left(\frac{t}{a+t}\right) I(t \neq 0) + \left(\frac{a}{a+t} \right)t, \quad t \in [0, \infty)
$$

Thereafter, taking the limit of $\rho_a(t)$, we obtain $\lim_{a \to 0} \rho_a(t) = I(t \neq 0)$ and $\lim_{a \to \infty} \rho_a(t) = t$, and when $a \approx 0$, the bridge between $\ell_0$ and $\ell_1$ becomes $\sum_{j=1}^{p} I(t \neq 0)(|\beta_j|)$, which is approximately equal to $\lVert \beta \rVert_0$ as specified before. Then, we reform the previous optimization problem to be a non-convex but more solvable function: $\tilde{\beta} = \arg\min_{\beta} \sum_{j=1}^{p} \rho_a(|\beta_j|)$ subject to $\beta \in C_\alpha$. Applying a local linear approximation on $\rho_a(|\beta_j|)$ gives the following result.

$$
\rho_a(|\beta_j|) \approx \rho_a(|\hat{\beta}_j|) + \rho_a'(|\hat{\beta}_j|) \big(|\beta_j| - |\hat{\beta}_j|\big), \quad
\rho_a'(|\hat{\beta}_j|) = \frac{a(a+1)}{(a + |\beta_j|)^2}
$$

Using the Lagrangian, the optimization problem further becomes 
$\tilde{\beta} = \arg\min_{\beta} \\{(\beta - \hat{\beta})^\top \Sigma^{-1} (\beta - \hat{\beta}) + \lambda_\alpha \sum_{j=1}^{p} \frac{|\beta_j|}{(a + |\hat{\beta_j}|)^2}\\}$, 
where $\lambda_\alpha$ is the Lagrange multiplier, which also acts as a penalty given a chosen $\alpha$. Since we are interested in the case that $a \to 0$, rewriting the function yields $\tilde{\beta} = \arg\min_{\beta} \\{(\beta - \hat{\beta})^\top \Sigma^{-1} (\beta - \hat{\beta}) + \lambda_\alpha \sum_{j=1}^{p} \frac{|\beta_j|}{|\hat{\beta}_j|^2}\\}$. The sequence of solutions to this equation can be solved through the least-angle regression (LARS) algorithm. The paper next exemplifies the case when $p < n$ and an improper flat prior is used for $\beta$. Under this situation, $\beta$ becomes a maximum likelihood estimator, and $\frac{1}{|\hat{\beta}_j|^2}$ can be viewed as the weight. The entire problem then reduces to an adaptive Lasso. Consequently, a non-informative flat prior when $p < n$ leads to a special case of the proposed approach, and some regularization is needed.

This paper also briefly points out the similarities and differences between the penalized credible regions approach and the Dantzig selector, which also aims to find a sparse model within a constrained region, but via $\tilde{\beta_{DS}} = \arg\min_{\beta} \lVert \beta\ \lVert_{\text{1}} \quad \text{subject to} \quad \beta \in D$, where $D = \\{\beta : \sup_{1 \le j \le p} |X_j^\top (Y - X\beta)| \le \text{some bound} \\}$. One major distinction is that the proposed approach uses a posterior credible region, whereas the Dantzig selector formulates a constraint by bounding the maximum correlation between a predictor and the residual vector.

### Part III: Asymptotic Theory
The paper then discusses some asymptotic theories of selection consistency for the joint credible sets. Theorem 1 states that in the case of a fixed dimension $p$ and under some general regularity assumptions, if $C_n \to \infty$ (the coverage goes to 1) and $n^{-1} C_n \to 0$, the joint credible regions approach shows selection consistency. That is, $P(A_n = A) \to 1$, where $A_n$ and $A$ are the chosen model and the unknown true model, respectively.

Because current datasets often have a huge dimension, we should also consider the situation where $p$ diverges. Likewise, Theorem 1 also holds when $p \to \infty$, but $p/n \to 0$ must be satisfied at the same time. Theorem 2 states that, if $p/n$ does not converge to zero, then the posterior mean $\hat{\beta}$ is not mean-square consistent, and thus the joint credible regions approach does not exhibit selection consistency in ultra-high dimensional settings. Modern data analysis more often focuses on $p \gg n$, where $p$ diverges at a faster rate than $n$ does. In this case, the paper additionally introduces the marginal credible regions approach, which also works in ultra-high dimensional settings.

The marginal credible regions approach builds rectangular credible sets for each parameter rather than a joint elliptical set based on the same posterior distribution, and the basic selection rule is $A_n = \\{ j : |\hat{\beta_j}| > t_{n,j} \\}$, where $t_{n,j}$ represents the threshold. Usually, $t_{n,j}$ is chosen to be $s_j t_n$, where $s_j$ could be the posterior standard deviation relative to the minimum standard deviation for $1 \le j \le p$. Corollary 2 is reached through some mathematical derivations. This corollary notes that when $\tau \to \infty$ and $\tau = O((n^2 \log p)^{1/3})$, the marginal credible regions approach is shown to be consistent up to $\log p = O(n^c)$, or $p = O(\exp\\{n^c\\})$, for some $0 < c < 1$. This conclusion further validates the selection consistency property for an exponentially growing dimension.

### Part IV: Simulations
A simulation study is performed to test how the penalized credible regions approach compares with other traditional methods. The study simulated 200 datasets from a linear regression model with the error term distributed as $N(0, 1)$, each having a sample size of 60 and a dimension $p \in \\{50, 500, 2000\\}$. In order to test how correlations among predictors affect the outcome, predictors $X_{i,j}$ are sampled from $N(0, 1)$ with AR(1) and lag-1 correlation, where $\rho \in \\{0.5, 0.9\\}$. The vector of true coefficients $\beta$ is set to be $(0_{10}^T, B_1^T, 0_{20}^T, B_2^T, 0_{p-40}^T)^T$, where the subscripts for zeros represent the length of each zero vector, and $B_1$ and $B_2$ are vectors of length 5 sampled uniformly from $(-5, 5)$.  

The methods that the experiment explores include the joint credible regions approach with $\tau \sim \text{Gamma}(0.01, 0.01)$, the marginal credible regions approach with $\tau \sim \text{Gamma}(0.01, 0.01)$, four versions of SSVS with combinations of fixed and random hyperparameters, frequentist Lasso, and OLS. For SSVS, we can fix the values of $\tau$ and $\pi$ using the known portion of true nonzero coefficients, or alternatively set $\tau \sim \text{Gamma}(0.01, 0.01)$ and $\pi \sim \text{Uniform}(0,1)$. For each dataset, 100,000 MCMC chains are performed and 5,000 are discarded. Using the credible regions approach, the ordered sets of predictors are generated by changing $\alpha$, while for SSVS, the ordered sets are obtained by varying the posterior inclusion thresholds. For other penalized methods, varying the tuning parameter yields ordered sets. For OLS, the ordered sets are defined by the magnitude of each standardized coefficient. The true coefficients are more likely to appear at the beginning of the ordering.

The performance of each method is measured by denoting each point on the ordered sets with a label from true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). Then, the Receiver-Operating Characteristic (ROC) curve is plotted with the false positive rate (FPR) on the x-axis and the true positive rate (TPR) on the y-axis. The FPR can also be denoted as $1 - \text{specificity}$, and the TPR can be denoted as recall or sensitivity.

$$
\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}} = \frac{\text{FP}}{N}, \quad \text{where $N$ is the total number of irrelevant predictors}
$$

$$
\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}} = \frac{\text{TP}}{P}, \quad \text{where $P$ is the total number of important predictors}
$$

The paper also proposes a Precision-Recall Curve (PRC), which plots the TPR (recall or sensitivity) on the x-axis and the ratio of TP to the total number declared as positive on the y-axis. This method is known to trade off between power and the false discovery rate (FDR), which conceptualizes the type I error rate in null hypothesis testing when conducting multiple comparisons. The larger the area under the ROC and PRC curves, the more accurate the variable selection is.

With everything set up, the paper plots the two curves under each setting. Below are the ROC and PRC curves for 200 datasets when $p = 500$. The first row is when $\rho = 0.5$, and the second row is when $\rho = 0.9$. It is obvious that under either the case of weakly correlated covariates or strongly correlated covariates, the credible sets approach performs better than the two types of SSVS. Further analysis suggests that although the computation time of the credible regions approach is slightly longer than that of Lasso and OLS, it takes much less time than SSVS. For $p = 2000$, each dataset takes more than 5 hours to compute using SSVS, whereas the proposed approach only takes less than 10 minutes. In addition, Lasso suffers from correlated variables more than any other methods. When $p = 50$, all methods give similar results except for the poor performance of OLS, which is potentially due to the instability of OLS when $p \approx n$. All these outcomes suggest that the penalized credible regions approach is more feasible in terms of computation time, accuracy, correlation, and so on.

At the end, the paper notes that under all settings, the joint credible regions approach outperforms the marginal one. A potential cause is that the joint approach uses associations among variables in a more efficient way. Nevertheless, the gap tends to decrease as the sample size increases.

## My Work and Extensions
In this part, I applied the penalized credible regions approach on the Boston housing dataset in a variety of settings, compared the outcomes with the frequentist Lasso approach, and compiled a list of variables selected using each method. The Boston housing dataset has a sample size of 506 and a dimension of 14. Among them, there are 13 predictors (`crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat`) and 1 response variable (`medv`), which is the median value of owner-occupied homes in $1{,}000$s. Because there are only 13 predictors, we need to generate spurious irrelevant variables for variable selection (Shin, 2017). Hence, I decided to generate noise variables in $\{20, 400\}$ to bring the dimension of the dataset to $\{33, 413\}$, representing a low-dimensional and a high-dimensional setting, respectively. Unlike the simulations in the paper, I only use this single dataset instead of simulating multiple datasets. Furthermore, I am curious about how independence and correlations among covariates influence the accuracy of each method, so I sampled both independent noise variables from a standard normal distribution and correlated noise variables from $\mathcal{N}(0, 1)$ with an AR(1) structure and lag-1 correlation of $\rho = 0.5$. To measure performance, I used the ROC curve mentioned in the paper. Also, I incorporated two new measurements, MS-O and MS-N, which count the average number of selected original variables and selected noise variables, respectively, in the first 20 ordered sets from the complete solution path.

For the credible sets approach, I utilized an online package called “BayesPen” and input the data, hyperparameters of priors, number of iterations equal to 5,000, and number of burn-in equal to 500. The residuals are assumed to be independently and identically distributed as $\mathcal{N}(0, \tau)$ with $\tau \sim \text{Gamma}(0.01, 0.01)$. The regression coefficients also have a $\mathcal{N}(0, \tau)$ distribution with $\tau \sim \text{Gamma}(0.01, 0.01)$. The function `BayesPen.lm` automatically generated ordered sets, and the outputs offered a complete solution path of adding variables (from zero to all variables) for both the joint and marginal approaches. For the Lasso regression, I simply used the `cv.glmnet` function with a varying tuning parameter $\lambda$ to create ordered sets.

Subsequently, I plotted the ROC curves of the complete solution paths by computing the false positive rate (FPR) and true positive rate (TPR). However, when calculating MS-O and MS-N, I used only the first 20 ordered sets from the complete paths. These first 20 ordered sets contain the variables that were added to the model the earliest. Ideally, the first 20 ordered sets should contain all 13 true variables. Below are the ROC plots under each setting.

![4](https://github.com/user-attachments/assets/db2586b7-7ac8-4dfc-8e7b-c79390bae6d9)

![5](https://github.com/user-attachments/assets/482651dc-dc3a-46e3-8346-913fb071291e)

![6](https://github.com/user-attachments/assets/4e1a406c-63d2-485f-a411-0748908e2b11)

![7](https://github.com/user-attachments/assets/e169a7f6-2904-489c-a0cd-8411252e7c81)

From the above plots, we can see that in most cases, the joint and marginal credible regions approaches have similar ROC curves and area under the curve (AUC). The joint approach performs slightly better than the marginal approach when variables are not correlated. Increasing dimensions and correlation does not reduce the AUC for these two methods. On the other hand, the Lasso approach underperforms both credible sets approaches most of the time, especially in a higher-dimensional setting, but it is surprising to see that in a low-dimensional setting, when the correlation increases from 0 to 0.5, the Lasso regression does not perform worse. This finding somehow contradicts the statement in the paper that Lasso suffers greatly from correlations among variables. Hence, this contradiction is worth further study. After that, I created a table for MS-O and MS-N.

![t1](https://github.com/user-attachments/assets/9764a7c2-3bc4-47ce-9a4e-1843ce812df8)

Table 1 compares the MS-O and MS-N for the first 20 ordered sets under each setting. The marginal method keeps selecting the most original predictors in all cases, followed by the joint approach, but the marginal approach also has a higher MS-N than the joint one. Moreover, I notice that when $p$ is large, the MS-O of credible sets methods improves and the MS-N drops as we increase the correlation. This suggests that the proposed approach yields an even more accurate outcome using correlated predictors. In a low-dimensional setting, the Lasso regression always selects the fewest original variables and the most noise variables. With a higher dimension, the average numbers of selected original variables of Lasso are down to 1.55 and 1.6, which are very low and unreasonable. Meanwhile, the MS-N is down to 0 for both independent and correlated variables. This result again assures us that Lasso is not as suitable as the credible sets approach when $p$ is large.

Regarding the computation time, when $p$ is small, the output of Lasso regression was generated almost immediately as I ran the code, while the credible sets methods took less than 5 seconds. When I increased the dimension to 413, the frequentist Lasso took a few seconds, and the credible sets method took less than 20 seconds. In general, the proposed approach took longer.

![WeChat65f90c3610079b261f801c58daaabe4e](https://github.com/user-attachments/assets/f46cbb10-cca5-455e-bb74-5e3988282f3c)

Next, in order to find out the exact variables selected, I examined the order path of each method. For convenience, I only selected the independent cases under low-dimensional and high-dimensional settings to study. A table including the first 13 variables added is shown above. Among these variables, "lstat", "rm", "ptratio", "dis", "nox", "black", and "tax" frequently appear at the beginning. These variables correspond to % lower status of the population, average number of rooms per dwelling, pupil-teacher ratio by town, weighted distances to five Boston employment centers, nitric oxides concentration (parts per 10 million), $1000 (B_k - 0.63)^2$ where $B_k$ is the proportion of blacks by town, and full-value property-tax rate per \$10,000. All methods selected a few noise variables, with the marginal approach in a high-dimensional setting selecting the most. Among those original variables, "age" was never selected, indicating that the proportion of owner-occupied units built prior to 1940 is not a necessary element determining the median value of owner-occupied homes in \$1000s. Moreover, we do notice that the marginal order path in high dimension is slightly different from the remaining three.

## Conclusion
In conclusion, for this project, I extensively summarized the paper of my choice and demonstrated how the credible regions approach could be a more efficient method than other frequentist methods and SSVS. Then, I extended the paper by including my own application on the Boston housing dataset using the proposed method and frequentist Lasso. In general, my application corresponds to the statements in the paper. The penalized credible regions approach works well in both low-dimensional and high-dimensional settings. With an increase in correlation among variables, the proposed approach yields even more desirable MS-O and MS-N. On the contrary, the frequentist Lasso shows poor performance when the dimension increases. However, in my experiment, the performance of Lasso did not worsen much when switching from independent variables to correlated variables. This point is worth further study. Finally, in this paper, I only compared the proposed method with the Lasso regression. For further studies, I could consider fitting the same dataset using MCMC and other common methods for a more comprehensive comparison.
